{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8w7-G3FRsne"
      },
      "outputs": [],
      "source": [
        "#Augmentation\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import sklearn\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
        "\n",
        "# Constants\n",
        "base_path = '/content/drive/MyDrive/Augmentation'  # NO train/test folder\n",
        "labels = ['NORMAL', 'PNEUMONIA']\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "VERBOSE = 1\n",
        "\n",
        "# Load Data\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "print(\"Starting data loading...\")\n",
        "for label in labels:\n",
        "    folderPath = os.path.join(base_path, label)\n",
        "    if not os.path.exists(folderPath):\n",
        "        print(f\"Missing folder: {folderPath}\")\n",
        "        continue\n",
        "    for filename in tqdm(os.listdir(folderPath), desc=f\"Loading {label} images\"):\n",
        "        img_path = os.path.join(folderPath, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "            x_data.append(img)\n",
        "            y_data.append(label)\n",
        "        else:\n",
        "            print(f\"Unreadable image: {img_path}\")\n",
        "\n",
        "x_data = np.array(x_data)\n",
        "y_data = np.array(y_data)\n",
        "x_data, y_data = sklearn.utils.shuffle(x_data, y_data, random_state=0)\n",
        "print(\"✅ Dataset loading complete.\")\n",
        "print(\"Total images loaded:\", len(x_data))\n",
        "\n",
        "# Class balance plot\n",
        "print(\"Plotting class distribution...\")\n",
        "sns.countplot(y_data)\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Display sample images\n",
        "for label in labels:\n",
        "    for idx, y in enumerate(y_data):\n",
        "        if y == label:\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.imshow(x_data[idx])\n",
        "            plt.title(label)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            break\n",
        "\n",
        "# Data Augmentation — safe version (no .fit)\n",
        "print(\"Applying image augmentation...\")\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "# No .fit(x_data) here to prevent memory issues\n",
        "\n",
        "# Split Data\n",
        "print(\"Splitting data into train and test...\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, test_size=0.10, random_state=0)\n",
        "\n",
        "print(\"Shapes - X: Train:\", x_train.shape, \" Test:\", x_test.shape)\n",
        "print(\"Shapes - Y: Train:\", y_train.shape, \" Test:\", y_test.shape)\n",
        "\n",
        "# One-hot encode labels\n",
        "print(\"Encoding labels...\")\n",
        "y_train = tf.keras.utils.to_categorical([labels.index(i) for i in y_train])\n",
        "y_test = tf.keras.utils.to_categorical([labels.index(i) for i in y_test])\n",
        "\n",
        "# Load EfficientNetB0\n",
        "print(\"Loading EfficientNetB0 model...\")\n",
        "effnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "# Add custom layers\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(effnet.output)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "model = tf.keras.models.Model(inputs=effnet.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "tensorboard = TensorBoard(log_dir='logs')\n",
        "checkpoint = ModelCheckpoint(\"effnet.h5\", monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=2, min_delta=0.001, verbose=VERBOSE)\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=VERBOSE,\n",
        "    callbacks=[tensorboard, checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Plot accuracy and loss\n",
        "print(\"Plotting training results...\")\n",
        "epochs = range(10)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "ax[0].plot(epochs, history.history['accuracy'], 'go-', label='Training Accuracy')\n",
        "ax[0].plot(epochs, history.history['val_accuracy'], 'ro-', label='Validation Accuracy')\n",
        "ax[0].set_title('Training & Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "\n",
        "ax[1].plot(epochs, history.history['loss'], 'g-o', label='Training Loss')\n",
        "ax[1].plot(epochs, history.history['val_loss'], 'r-o', label='Validation Loss')\n",
        "ax[1].set_title('Training & Validation Loss')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating on test set...\")\n",
        "result = model.evaluate(x_test, y_test)\n",
        "print(\"✅ Test Loss:\", result[0])\n",
        "print(\"✅ Test Accuracy:\", result[1] * 100, \"%\")\n",
        "\n",
        "# Predictions\n",
        "print(\"Generating predictions...\")\n",
        "predictions = model.predict(x_test)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\\n\", classification_report(true_classes, pred_classes, target_names=labels))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"Plotting confusion matrix...\")\n",
        "cf_matrix = confusion_matrix(true_classes, pred_classes)\n",
        "sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ]
    }
  ]
}